[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are from Beijing Normal University. We have developed more than 50 tasks based on psychological paradigms, in order to explore the structure of human cognition."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cognitive Structure Project",
    "section": "",
    "text": "structure\n\n\ncognition\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2022\n\n\nYifei Cao, Liang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstructure\n\n\nwm\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2022\n\n\nLiang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata-check\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nLiang Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/check-raw-data/index.html",
    "href": "posts/check-raw-data/index.html",
    "title": "Check Raw Data",
    "section": "",
    "text": "Here we check raw data from several special tasks. Especially check the factors influencing reliability, internal consistency of each task"
  },
  {
    "objectID": "posts/check-raw-data/index.html#schulte-grid-舒尔特方格",
    "href": "posts/check-raw-data/index.html#schulte-grid-舒尔特方格",
    "title": "Check Raw Data",
    "section": "Schulte Grid (舒尔特方格)",
    "text": "Schulte Grid (舒尔特方格)\n\n\nCode\ntargets::tar_load(data_valid_SchulteMed, store = here::here(\"preproc/_targets\"))\nrt_by_resp <- data_valid_SchulteMed |>\n  mutate(\n    raw_parsed = map(\n      raw_parsed,\n      ~ . |> mutate(resp = as.integer(resp))\n    )\n  ) |>\n  unnest(raw_parsed) |>\n  group_by(user_id, game_time) |>\n  mutate(resp_adj = ifelse(acc == 0, NA, resp)) |>\n  fill(resp_adj, .direction = \"up\") |>\n  ungroup() |>\n  drop_na() |>\n  group_by(user_id, game_version, game_time, resp_adj) |>\n  summarise(rt = sum(rt) / 1000, .groups = \"drop\") |>\n  filter(rt < 300)\nrt_by_resp |>\n  ggplot(aes(resp_adj, rt, color = game_version)) +\n  geom_point(shape = \".\") +\n  geom_smooth() +\n  scale_y_log10() +\n  scale_color_brewer(palette = \"Paired\") +\n  labs(x = \"\", y = \"Response Time (s)\", color = \"Version\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html",
    "href": "posts/explore-struct-wm/index.html",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(BayesFM)\nThe include tasks:"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#traditional",
    "href": "posts/explore-struct-wm/index.html#traditional",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Traditional",
    "text": "Traditional\n\n\nCode\nnfactors_test <- psych::nfactors(indices_memory)\n\n\n\n\n\n\nCorrelated latent factors\n\n\n\nCode\nfit <- psych::fa(indices_memory, 3)\npsych::fa.diagram(fit)\n\n\n\n\n\n\nBifactor model\n\n\n\nCode\nfit_bifac <- psych::omega(indices_memory, 3, plot = FALSE)\npsych::omega.diagram(fit_bifac)"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#bayesian-factor-analysis",
    "href": "posts/explore-struct-wm/index.html#bayesian-factor-analysis",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Bayesian Factor Analysis",
    "text": "Bayesian Factor Analysis\n\n\nCode\nmcmc <- indices_memory |> \n  mutate(across(.fns = ~ scale(.)[, 1])) |> \n  befa(verbose = FALSE) |> \n  post.column.switch() |> \n  post.sign.switch()\n\n\n\n\nCode\nhppm <- summary(mcmc, what = \"hppm\")\nhppm |> \n  pluck(\"alpha\", \"m1\") |> \n  as_tibble(rownames = \"alpha_term\") |> \n  separate(alpha_term, c(NA, \"game_index\"), sep = \":\") |> \n  mutate(game_index = reorder(game_index, dedic)) |> \n  ggplot(aes(game_index, dedic)) +\n  geom_tile(aes(fill = mean)) +\n  geom_text(aes(label = round(mean, 2)), color = \"white\") +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"Term\", y = \"Factor\", \n       title = str_c(\"Posterior Probability: \", round(hppm$hppm$prob, 2), \n                     \", with \", hppm$hppm$nfac, \" factors\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/Large-scale meta analysis/index.html",
    "href": "posts/Large-scale meta analysis/index.html",
    "title": "Extracting Cognitive Factors from Large-Scale Meta-Anslysis Data",
    "section": "",
    "text": "To summarize briefly, I followed the step of Beam et al. (2021, Nature Neuroscience) to find evidence for the structure of human cognition. Firstly, filtering all studies that included the testing paradigms that consisted with those used in our project from the database of Neurosynth, I conducted 28 meta-analysis separately on each group of fMRI studies. Thanks to NiMARE (a python research environment for neuroimaging meta-analysis), I could easily perform Multilevel Kernel Density Analysis (MKDA) on hundreds fMRI studies without manually coding.\nHere is the summarize table for the number of included fMRI studies in each meta-analysis."
  },
  {
    "objectID": "posts/Large-scale meta analysis/index.html#traditional",
    "href": "posts/Large-scale meta analysis/index.html#traditional",
    "title": "Extracting Cognitive Factors from Large-Scale Meta-Anslysis Data",
    "section": "Traditional",
    "text": "Traditional\n\n\nCode\nnfactors_test <- psych::nfactors(cor_nonzero)\n\n\n\n\n\n\nCorrelated latent factors\n\n\n\nCode\nfit <- psych::fa(cor_nonzero, 4)\npsych::fa.diagram(fit)\n\n\n\n\n\n\nBifactor model\n\n\n\nCode\nfit_bifac <- psych::omega(total_nonzero, 3, plot = FALSE)\npsych::omega.diagram(fit_bifac)"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#model-from-efa",
    "href": "posts/explore-struct-wm/index.html#model-from-efa",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Model from EFA",
    "text": "Model from EFA\nFirstly, the model from EFA is tested.\n\nSpatial-object association test (宇宙黑洞) included\n\n\n\nCode\nfitted1 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_memory, std.lv = TRUE, std.ov = TRUE\n)\nsemPlot::semPaths(\n  fitted1, what = \"std\", edge.color = \"black\", rotation = 2,\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, \n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n                                                      \n                                                  Used       Total\n  Number of observations                           314         468\n                                                                  \nModel Test User Model:\n                                                      \n  Test statistic                               104.853\n  Degrees of freedom                                41\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               809.797\n  Degrees of freedom                                55\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.915\n  Tucker-Lewis Index (TLI)                       0.887\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4543.033\n  Loglikelihood unrestricted model (H1)      -4490.606\n                                                      \n  Akaike (AIC)                                9136.065\n  Bayesian (BIC)                              9229.800\n  Sample-size adjusted Bayesian (BIC)         9150.508\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.070\n  90 Percent confidence interval - lower         0.054\n  90 Percent confidence interval - upper         0.087\n  P-value RMSEA <= 0.05                          0.022\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.051\n\n\n\nSpatial-object association test (宇宙黑洞) excluded\n\n\n\nCode\nfitted2 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_memory, std.lv = TRUE, std.ov = TRUE\n)\nsemPlot::semPaths(\n  fitted2, what = \"std\", edge.color = \"black\", rotation = 2,\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, \n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n                                                      \n                                                  Used       Total\n  Number of observations                           322         468\n                                                                  \nModel Test User Model:\n                                                      \n  Test statistic                                80.279\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               752.317\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.932\n  Tucker-Lewis Index (TLI)                       0.904\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4227.955\n  Loglikelihood unrestricted model (H1)      -4187.816\n                                                      \n  Akaike (AIC)                                8501.910\n  Bayesian (BIC)                              8588.725\n  Sample-size adjusted Bayesian (BIC)         8515.772\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.068\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.087\n  P-value RMSEA <= 0.05                          0.051\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.047"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#task-structure-model",
    "href": "posts/explore-struct-wm/index.html#task-structure-model",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Task Structure Model",
    "text": "Task Structure Model\n\nThe covariance estimate for latent variables is not correct. The estimated correlation between “Simple” and “Complex” is 1.02.\n\n\n\nCode\nfitted3 <- lavaan::cfa(\n  'Complex =~ `打靶场` + `蝴蝶照相机` + `幸运小球`\n  Nback =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  Simple =~ `密码箱` + `顺背数PRO` + `位置记忆PRO`',\n  indices_memory, std.lv = TRUE, std.ov = TRUE\n)\nsemPlot::semPaths(\n  fitted3, what = \"std\", edge.color = \"black\", rotation = 2,\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted3, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 19 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n                                                      \n                                                  Used       Total\n  Number of observations                           322         468\n                                                                  \nModel Test User Model:\n                                                      \n  Test statistic                               163.052\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               752.317\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.815\n  Tucker-Lewis Index (TLI)                       0.739\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4269.341\n  Loglikelihood unrestricted model (H1)      -4187.816\n                                                      \n  Akaike (AIC)                                8584.683\n  Bayesian (BIC)                              8671.498\n  Sample-size adjusted Bayesian (BIC)         8598.545\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.113\n  90 Percent confidence interval - lower         0.096\n  90 Percent confidence interval - upper         0.130\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.076"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#input-material-model",
    "href": "posts/explore-struct-wm/index.html#input-material-model",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Input Material Model",
    "text": "Input Material Model\n\nNot fitted well.\n\n\n\nCode\nfitted4 <- lavaan::cfa(\n  'Verbal =~ 数字卡片 + 文字卡片 + 幸运小球 + 密码箱 + 顺背数PRO\n  Spatial =~ 格子卡片 + 打靶场 + 蝴蝶照相机 + 位置记忆PRO\n  Object =~ 美术卡片 + 宇宙黑洞',\n  indices_memory, std.lv = TRUE, std.ov = TRUE\n)\nsemPlot::semPaths(\n  fitted4, what = \"std\", edge.color = \"black\", rotation = 2,\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted4, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n                                                      \n                                                  Used       Total\n  Number of observations                           314         468\n                                                                  \nModel Test User Model:\n                                                      \n  Test statistic                               265.138\n  Degrees of freedom                                41\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               809.797\n  Degrees of freedom                                55\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.703\n  Tucker-Lewis Index (TLI)                       0.602\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4623.176\n  Loglikelihood unrestricted model (H1)      -4490.606\n                                                      \n  Akaike (AIC)                                9296.351\n  Bayesian (BIC)                              9390.086\n  Sample-size adjusted Bayesian (BIC)         9310.793\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.132\n  90 Percent confidence interval - lower         0.117\n  90 Percent confidence interval - upper         0.147\n  P-value RMSEA <= 0.05                          0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.098"
  },
  {
    "objectID": "posts/check-raw-data/index.html#reasoning-推理类题目",
    "href": "posts/check-raw-data/index.html#reasoning-推理类题目",
    "title": "Check Raw Data",
    "section": "Reasoning (推理类题目)",
    "text": "Reasoning (推理类题目)\n\n\nCode\ntargets::tar_load(data_valid_DRA, store = here::here(\"preproc/_targets\"))\ndata_valid_DRA |> \n  filter(course_name == \"清华大学认知实验D\") |> \n  unnest(raw_parsed) |> \n  mutate(item = as.numeric(as_factor(itemid))) |> \n  # group_by(item) |> \n  # filter(between(mean(acc == 1), 0.6, 0.9)) |>\n  # ungroup() |> \n  filter(acc != -1) |> \n  pivot_wider(\n    id_cols = user_id,\n    names_from = item,\n    values_from = acc\n  ) |> \n  psycModel::reliability_summary(-user_id)\n\n\nSome items ( 2 8 10 15 20 21 23 25 29 30 ) were negatively correlated with the total scale and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\n\n\n\n\u001b[4mModel Summary\u001b[24m\n\u001b[mModel Type = Reliability Analysis\u001b[m\n\u001b[mDimensionality = multi-dimensionality\u001b[m\n\n\u001b[4mComposite Reliability Measures\u001b[24m\n──────────────────────────────────────────────────────────\n  Alpha  Alpha.Std    G.6  Omega.Hierarchical  Omega.Total\n──────────────────────────────────────────────────────────\n  0.285      0.366  0.959               0.484        0.675\n──────────────────────────────────────────────────────────\n\n\u001b[4mItem Reliability (item dropped)\u001b[24m\n─────────────────────────────────\n  Var  Alpha  Alpha.Std  G6 (smc)\n─────────────────────────────────\n    1  0.262      0.351     0.797\n    2  0.299      0.398     0.866\n    3  0.255      0.331     0.752\n    4  0.249      0.326     0.781\n    5  0.264      0.326     0.902\n    6  0.274      0.350     0.788\n    7  0.240      0.316     0.799\n    8  0.288      0.370     0.813\n    9  0.242      0.339     0.774\n   10  0.265      0.354     0.966\n   11  0.199      0.281     0.813\n   12  0.256      0.334     0.835\n   13  0.228      0.305     0.751\n   14  0.177      0.265     0.812\n   15  0.315      0.398     0.809\n   16  0.292      0.361     0.769\n   17  0.235      0.326     0.777\n   18  0.275      0.365     0.773\n   19  0.281      0.349     0.781\n   20  0.281      0.364     0.763\n   21  0.279      0.371     0.794\n   22  0.266      0.344     0.843\n   23  0.302      0.382     0.800\n   24  0.361      0.414     0.978\n   25  0.314      0.388     0.779\n   26  0.286      0.372     0.801\n   27  0.237      0.339     0.770\n   28  0.304      0.379     0.915\n   29  0.366      0.429     0.807\n   30  0.391      0.452     0.878\n─────────────────────────────────\n\n\u001b[4mDescriptive Statistics Table:\u001b[24m\n\n───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n  Var   mean     sd           1           2           3           4           5           6           7           8           9          10          11          12          13          14          15          16          17          18          19          20          21          22          23          24          25          26          27          28          29\n───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n    1  0.857  0.355                                                                                                                                                                                                                                                                                                                                                            \n    2  0.971  0.169  -0.070                                                                                                                                                                                                                                                                                                                                                    \n    3  0.914  0.284   0.167      -0.053                                                                                                                                                                                                                                                                                                                                        \n    4  0.829  0.382   0.031       0.377       0.132                                                                                                                                                                                                                                                                                                                            \n    5  0.971  0.169  -0.070      -0.029      -0.053      -0.078                                                                                                                                                                                                                                                                                                                \n    6  0.914  0.284  -0.125      -0.053       0.271       0.402      -0.053                                                                                                                                                                                                                                                                                                    \n    7  0.829  0.382  -0.186      -0.078       0.132      -0.006       0.377       0.132                                                                                                                                                                                                                                                                                        \n    8  0.686  0.471  -0.101      -0.116       0.013       0.019      -0.116       0.013       0.182                                                                                                                                                                                                                                                                            \n    9  0.629  0.490   0.193      -0.132      -0.024       0.121       0.223      -0.235       0.121      -0.011                                                                                                                                                                                                                                                                \n   10  0.829  0.382   0.248      -0.078      -0.139      -0.006      -0.078       0.132      -0.006       0.182      -0.036                                                                                                                                                                                                                                                    \n   11  0.829  0.382   0.031      -0.078       0.132      -0.006       0.377       0.132       0.397       0.182      -0.036       0.195                                                                                                                                                                                                                                        \n   12  0.829  0.382  -0.186      -0.078       0.132       0.397       0.377       0.402       0.195      -0.145       0.121      -0.207       0.195                                                                                                                                                                                                                            \n   13  0.886  0.323   0.110      -0.062       0.532       0.075      -0.062       0.211       0.313       0.144       0.096      -0.163       0.075       0.075                                                                                                                                                                                                                \n   14  0.794  0.410   0.199      -0.089       0.355      -0.045       0.342       0.098       0.337       0.114       0.198       0.146       0.718 ***   0.146       0.266                                                                                                                                                                                                    \n   15  0.706  0.462   0.096      -0.112       0.027      -0.299      -0.112      -0.201      -0.129       0.106       0.023       0.040      -0.299      -0.129       0.165      -0.169                                                                                                                                                                                        \n   16  0.647  0.485   0.215       0.236      -0.013       0.142       0.236      -0.230      -0.180       0.015       0.052      -0.019       0.142      -0.019       0.112       0.233       0.064                                                                                                                                                                            \n   17  0.618  0.493  -0.156      -0.137       0.182      -0.047       0.221      -0.031       0.271       0.103      -0.121      -0.047       0.112       0.271       0.464       0.048       0.289       0.052                                                                                                                                                                \n   18  0.364  0.489   0.144      -0.234       0.020       0.030       0.134       0.020      -0.134      -0.050       0.440      -0.032      -0.032       0.193      -0.105       0.193      -0.187      -0.083      -0.293                                                                                                                                                    \n   19  0.576  0.502  -0.192       0.206       0.155       0.549       0.206       0.368       0.072      -0.166       0.116      -0.021      -0.021       0.390      -0.131       0.072      -0.299       0.116      -0.065       0.139                                                                                                                                        \n   20  0.485  0.508   0.241      -0.182       0.096      -0.171       0.171      -0.115      -0.014       0.112       0.229       0.410       0.072      -0.171      -0.197       0.300      -0.020      -0.023      -0.335       0.527      -0.026                                                                                                                            \n   21  0.152  0.364  -0.057       0.075      -0.454      -0.020       0.075      -0.160       0.199      -0.089      -0.032       0.179       0.179      -0.239      -0.102      -0.020       0.095      -0.032       0.168      -0.144       0.021      -0.072                                                                                                                \n   22  0.344  0.483   0.075       0.130      -0.219       0.011       0.130       0.233       0.011      -0.080      -0.255       0.311       0.130       0.179       0.075      -0.158       0.062      -0.119       0.331      -0.108      -0.025      -0.152       0.232                                                                                                    \n   23  0.367  0.490  -0.109       0.141       0.023      -0.138       0.141      -0.438      -0.031      -0.049       0.148      -0.217      -0.031      -0.311       0.023       0.035       0.045       0.148       0.198       0.106      -0.033       0.033       0.217      -0.098                                                                                        \n   24  0.393  0.497  -0.090       0.155       0.042      -0.007      -0.239       0.042       0.119       0.023      -0.316      -0.007      -0.090      -0.115       0.279      -0.007       0.084      -0.011       0.106      -0.240      -0.042      -0.162       0.198      -0.185      -0.198                                                                            \n   25  0.333  0.480   0.074       0.139       0.000       0.295      -0.277       0.000       0.074       0.287       0.000      -0.067      -0.147      -0.270       0.250      -0.067       0.000      -0.108       0.107      -0.333      -0.053       0.000       0.067      -0.060       0.108       0.213                                                                \n   26  0.630  0.492  -0.104      -0.150      -0.027       0.112      -0.150       0.217      -0.104      -0.162      -0.217      -0.168       0.328       0.424      -0.027       0.029      -0.054      -0.112      -0.012       0.054      -0.012      -0.240      -0.029       0.279      -0.047       0.012      -0.108                                                    \n   27  0.556  0.506   0.466      -0.175       0.395       0.047      -0.175      -0.079      -0.163      -0.254       0.158      -0.043       0.256      -0.043       0.158       0.149       0.316       0.086       0.169       0.000       0.017      -0.100       0.043       0.019       0.223      -0.017       0.000       0.240                                        \n   28  0.360  0.490   0.021      -0.272       0.021      -0.236       0.153       0.021       0.100      -0.200       0.157       0.100       0.327      -0.042       0.021      -0.042       0.042      -0.306      -0.007       0.132      -0.238       0.113       0.250       0.164       0.068      -0.272      -0.389       0.215       0.161                            \n   29  0.208  0.415   0.194       0.107      -0.116      -0.116       0.107      -0.116      -0.046       0.145       0.145       0.229      -0.046      -0.242      -0.116       0.011      -0.238       0.185      -0.146       0.026      -0.399       0.103      -0.263      -0.059      -0.017      -0.434      -0.185      -0.290      -0.352       0.026                \n   30  0.652  0.487  -0.283      -0.156      -0.283      -0.012      -0.156      -0.012      -0.094       0.042       0.042      -0.094      -0.335       0.058      -0.283      -0.164      -0.024      -0.342      -0.151       0.150       0.096      -0.032      -0.147       0.164      -0.163      -0.280       0.024       0.233      -0.334      -0.233      -0.058    \n───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/check-raw-data/index.html#forward-word-span-过目不忘",
    "href": "posts/check-raw-data/index.html#forward-word-span-过目不忘",
    "title": "Check Raw Data",
    "section": "Forward Word Span (过目不忘)",
    "text": "Forward Word Span (过目不忘)\n\n\nCode\ntargets::tar_load(data_valid_FWSPro, store = here::here(\"preproc/_targets\"))\nchrs_freq <- read_tsv(\"CharFreq.txt\", skip = 5)\nchrs_used <- readxl::read_excel(\"过目不忘-汉字库.xlsx\") |>\n  left_join(chrs_freq, by = \"汉字\")\ndata_adj_acc <- data_valid_FWSPro |>\n  unnest(raw_parsed) |>\n  group_by(user_id, game_time) |>\n  mutate(trial = row_number()) |>\n  ungroup() |>\n  mutate(\n    across(\n      c(stim, resp),\n      str_split,\n      pattern = \"-\"\n    )\n  ) |>\n  unnest(c(stim, resp)) |>\n  left_join(\n    select(chrs_used, stim = 汉字, stim_id = ID, stim_freq = 序列号),\n    by = \"stim\"\n  ) |>\n  left_join(\n    select(chrs_used, resp = 汉字, resp_id = ID),\n    by = \"resp\"\n  ) |>\n  separate(stim_id, c(\"stim_phon\", \"stim_form\"), convert = TRUE) |>\n  separate(resp_id, c(\"resp_phon\", \"resp_form\"), convert = TRUE) |>\n  mutate(\n    acc = case_when(\n      stim == resp ~ \"正确\",\n      stim_phon == resp_phon ~ \"同音字\",\n      stim_phon != resp_phon ~ \"错误\"\n    )\n  )\ndata_adj_acc |>\n  group_by(stim, stim_phon, stim_freq, acc) |>\n  summarise(n = n(), .groups = \"drop_last\") |>\n  mutate(prop = n / sum(n)) |>\n  ungroup() |>\n  ggplot(aes(stim, prop, fill = acc)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(label = scales::label_percent(accuracy = 1)(prop)),\n    position = position_stack(vjust = 0.5),\n    color = \"white\"\n  ) +\n  geom_text(\n    aes(label = stim_freq),\n    y = 0\n  ) +\n  scale_fill_brewer(palette = \"Accent\") +\n  facet_wrap(~ stim_phon, scales = \"free_x\", nrow = 1) +\n  labs(x = \"\", y = \"\", fill = \"\") +\n  # scale_y_continuous(expand = c(0, 0)) +\n  ggthemes::theme_hc() +\n  theme(\n    axis.text.x = element_text(family = \"SimHei\"),\n    legend.text = element_text(family = \"SimHei\")\n  )\n\n\n\n\n\nHomophone Selection Proportion"
  }
]